{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69fba6b9-dae8-49e5-8c99-6709b0ebd01b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Bronze Layer\n",
    "#--->Read the data from the source and Write the data to the dbfs in the delta lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "825a6c34-4d7a-41e5-8af2-a24a56231ed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4282d872-e49b-4add-939b-e63b762a474e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"EcomDataPipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cbf2c6-dfe3-4440-87ef-f623d060f796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def files_exist(path):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    return len(files) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97727fed-7db9-49cf-b3d9-982fb03a7da6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found in the path: abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/users-raw-2/to_proccesed/\n"
     ]
    }
   ],
   "source": [
    "#For User Data Batch Processing \n",
    "source_User_path = \"abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/users-raw-2/to_proccesed/\"\n",
    "delta_User_table_path = \"/mnt/delta/tables/bronze/users\"\n",
    "processed_User_path = \"abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/users-raw-2/proccesed/\"\n",
    "\n",
    "\n",
    "if files_exist(source_User_path):\n",
    "\n",
    "    UserDF=spark.read.format(\"parquet\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(source_User_path)\n",
    "\n",
    "\n",
    "    # Check if the Delta table exists\n",
    "    if DeltaTable.isDeltaTable(spark, delta_User_table_path):\n",
    "        # If the table exists, append the new data\n",
    "        UserDF.write.format(\"delta\").mode(\"append\").save(delta_User_table_path)\n",
    "    else:\n",
    "        # If the table does not exist, create it using overwrite mode\n",
    "        UserDF.write.format(\"delta\").mode(\"overwrite\").save(delta_User_table_path)\n",
    "\n",
    "    # Move processed files to the processed directory\n",
    "    def move_files(src_path, dest_path):\n",
    "        files = dbutils.fs.ls(src_path)\n",
    "        for file in files:\n",
    "            file_path = file.path\n",
    "            dest_file_path = dest_path + file.name\n",
    "            dbutils.fs.mv(file_path, dest_file_path)\n",
    "\n",
    "    move_files(source_User_path, processed_User_path)\n",
    "\n",
    "    print(\"New files have been appended to the Delta table and moved to the processed directory.\")\n",
    "\n",
    "else:\n",
    "    print(f\"No files found in the path: {source_User_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03e50cb-ba5a-4d31-8f6a-db006f301c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New files have been appended to the Delta table and moved to the processed directory.\n"
     ]
    }
   ],
   "source": [
    "#For Buyer Data Batch Processing \n",
    "source_Buyer_path = \"abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/buyers-raw-2/to_proccesed/\"\n",
    "delta_Buyer_table_path = \"/mnt/delta/tables/bronze/buyers\"\n",
    "processed_Buyer_path = \"abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/buyers-raw-2/proccesed/\"\n",
    "\n",
    "\n",
    "if files_exist(source_Buyer_path):\n",
    "\n",
    "    BuyerDF=spark.read.format(\"parquet\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(source_Buyer_path)\n",
    "\n",
    "\n",
    "    # Check if the Delta table exists\n",
    "    if DeltaTable.isDeltaTable(spark, delta_Buyer_table_path):\n",
    "        # If the table exists, append the new data\n",
    "        BuyerDF.write.format(\"delta\").mode(\"append\").save(delta_Buyer_table_path)\n",
    "    else:\n",
    "        # If the table does not exist, create it using overwrite mode\n",
    "        BuyerDF.write.format(\"delta\").mode(\"overwrite\").save(delta_Buyer_table_path)\n",
    "\n",
    "    # Move processed files to the processed directory\n",
    "    def move_files(src_path, dest_path):\n",
    "        files = dbutils.fs.ls(src_path)\n",
    "        for file in files:\n",
    "            file_path = file.path\n",
    "            dest_file_path = dest_path + file.name\n",
    "            dbutils.fs.mv(file_path, dest_file_path)\n",
    "\n",
    "    move_files(source_Buyer_path, processed_Buyer_path)\n",
    "\n",
    "    print(\"New files have been appended to the Delta table and moved to the processed directory.\")\n",
    "\n",
    "else:\n",
    "    print(f\"No files found in the path: {source_Buyer_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebec6b01-d7a0-4552-a257-0f34d038630b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New files have been appended to the Delta table and moved to the processed directory.\n"
     ]
    }
   ],
   "source": [
    "#For Seller Data\n",
    "source_Seller_path = \"abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/sellers-raw-2/to_proccesed/\"\n",
    "delta_Seller_table_path = \"/mnt/delta/tables/bronze/sellers\"\n",
    "processed_Seller_path = \"abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/sellers-raw-2/proccesed/\"\n",
    "\n",
    "\n",
    "if files_exist(source_Seller_path):\n",
    "\n",
    "    SellerDF=spark.read.format(\"parquet\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(source_Seller_path)\n",
    "\n",
    "\n",
    "    # Check if the Delta table exists\n",
    "    if DeltaTable.isDeltaTable(spark, delta_Seller_table_path):\n",
    "        # If the table exists, append the new data\n",
    "        SellerDF.write.format(\"delta\").mode(\"append\").save(delta_Seller_table_path)\n",
    "    else:\n",
    "        # If the table does not exist, create it using overwrite mode\n",
    "        SellerDF.write.format(\"delta\").mode(\"overwrite\").save(delta_Seller_table_path)\n",
    "\n",
    "    # Move processed files to the processed directory\n",
    "    def move_files(src_path, dest_path):\n",
    "        files = dbutils.fs.ls(src_path)\n",
    "        for file in files:\n",
    "            file_path = file.path\n",
    "            dest_file_path = dest_path + file.name\n",
    "            dbutils.fs.mv(file_path, dest_file_path)\n",
    "\n",
    "    move_files(source_Seller_path, processed_Seller_path)\n",
    "\n",
    "    print(\"New files have been appended to the Delta table and moved to the processed directory.\")\n",
    "\n",
    "else:\n",
    "    print(f\"No files found in the path: {source_Seller_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdc1eab-5763-4520-b023-26c870ba3cbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New files have been appended to the Delta table and moved to the processed directory.\n"
     ]
    }
   ],
   "source": [
    "#For countries Data\n",
    "source_Country_path = \"abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/countries-raw-2/to_proccesed/\"\n",
    "delta_Country_table_path = \"/mnt/delta/tables/bronze/countries\"\n",
    "processed_Country_path = \"abfss://landing-zone-2@ecomadlsshivasai.dfs.core.windows.net/countries-raw-2/proccesed/\"\n",
    "\n",
    "\n",
    "if files_exist(source_Country_path):\n",
    "    CountryDF=spark.read.format(\"parquet\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(source_Country_path)\n",
    "\n",
    "\n",
    "    # Check if the Delta table exists\n",
    "    if DeltaTable.isDeltaTable(spark, delta_Country_table_path):\n",
    "        # If the table exists, append the new data\n",
    "        CountryDF.write.format(\"delta\").mode(\"append\").save(delta_Country_table_path)\n",
    "    else:\n",
    "        # If the table does not exist, create it using overwrite mode\n",
    "        CountryDF.write.format(\"delta\").mode(\"overwrite\").save(delta_Country_table_path)\n",
    "\n",
    "    # Move processed files to the processed directory\n",
    "    def move_files(src_path, dest_path):\n",
    "        files = dbutils.fs.ls(src_path)\n",
    "        for file in files:\n",
    "            file_path = file.path\n",
    "            dest_file_path = dest_path + file.name\n",
    "            dbutils.fs.mv(file_path, dest_file_path)\n",
    "\n",
    "    move_files(source_Country_path, processed_Country_path)\n",
    "\n",
    "    print(\"New files have been appended to the Delta table and moved to the processed directory.\")\n",
    "else:\n",
    "    print(f\"No files found in the path: {source_Country_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62c449b-a5cb-44bb-9562-78df68370644",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the Delta table files\n",
    "# dbutils.fs.rm(\"/mnt/delta/tables/bronze/users\", recurse=True)\n",
    "# dbutils.fs.rm(\"/mnt/delta/tables/bronze/buyers\", recurse=True)\n",
    "# dbutils.fs.rm(\"/mnt/delta/tables/bronze/sellers\", recurse=True)\n",
    "# dbutils.fs.rm(\"/mnt/delta/tables/bronze/countries\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b1b72d-cfce-4793-b154-90906b5e6e4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Delta table at /mnt/delta/tables/bronze/users has 59349 rows.\nThe Delta table at /mnt/delta/tables/bronze/buyers has 62 rows.\nThe Delta table at /mnt/delta/tables/bronze/sellers has 73 rows.\nThe Delta table at /mnt/delta/tables/bronze/countries has 19 rows.\n"
     ]
    }
   ],
   "source": [
    "User_table_path = \"/mnt/delta/tables/bronze/users\"\n",
    "Buyer_table_path = \"/mnt/delta/tables/bronze/buyers\"\n",
    "Seller_table_path = \"/mnt/delta/tables/bronze/sellers\"\n",
    "Country_table_path = \"/mnt/delta/tables/bronze/countries\"\n",
    "\n",
    "# Function to get the row count of a Delta table\n",
    "def get_row_count(delta_table_path):\n",
    "    df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    return df.count()\n",
    "\n",
    "# Get the number of rows in each table\n",
    "num_rows_User = get_row_count(User_table_path)\n",
    "num_rows_Buyer = get_row_count(Buyer_table_path)\n",
    "num_rows_Seller = get_row_count(Seller_table_path)\n",
    "num_rows_Country = get_row_count(Country_table_path)\n",
    "\n",
    "# Print the counts\n",
    "print(f\"The Delta table at {User_table_path} has {num_rows_User} rows.\")\n",
    "print(f\"The Delta table at {Buyer_table_path} has {num_rows_Buyer} rows.\")\n",
    "print(f\"The Delta table at {Seller_table_path} has {num_rows_Seller} rows.\")\n",
    "print(f\"The Delta table at {Country_table_path} has {num_rows_Country} rows.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2343343745007443,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze-Layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
